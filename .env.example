# Local AI Coder Configuration Template
# Copy this file to .env and customize the values

# ===== LLM Provider Selection =====
# "huggingface" = Fully local, no external server needed (default)
# "ollama" = Requires Ollama server, access to latest models
LLM_PROVIDER="ollama"

# ===== Ollama Configuration =====
# Used when LLM_PROVIDER="ollama" (external server required)
OLLAMA_MODEL="gpt-oss:20b"
OLLAMA_BASE_URL="http://localhost:11434"

# Optional: For remote Ollama servers with authentication
# OLLAMA_API_KEY="your_api_key_here"

# ===== HuggingFace Configuration =====
# Used when LLM_PROVIDER="huggingface" (fully local, auto-downloads models)
HF_MODEL_ID="openai/gpt-oss-20b"

# Device selection for HuggingFace models
# "cpu" = CPU inference (slower, works everywhere)
# "cuda" = NVIDIA GPU acceleration (faster, requires CUDA) 
# "mps" = Apple Silicon Metal acceleration (faster on M1/M2/M3)
# Leave empty for auto-detection (recommended)
# HF_DEVICE="mps"

# ===== Examples =====
# Ollama models:
# MODEL="llama3.1:8b"
# MODEL="mistral:7b" 
# MODEL="codellama:13b"

# HuggingFace models (smaller models work better for most systems):
# HF_MODEL_ID="microsoft/DialoGPT-small"
# HF_MODEL_ID="microsoft/DialoGPT-medium"  
# HF_MODEL_ID="distilgpt2"
# HF_MODEL_ID="gpt2"

# For remote Ollama server:
# OLLAMA_BASE_URL="http://your-server:11434"

# For specific device targeting:
# HF_DEVICE="cuda"  # NVIDIA GPU
# HF_DEVICE="mps"   # Apple Silicon Metal
# HF_DEVICE="cpu"   # CPU only
